{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2347676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = 'https://www.amazon.in/s'\n",
    "    params = {'k': product}\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', {'class': 'sg-col-inner'})\n",
    "\n",
    "        for product in products:\n",
    "            product_name = product.find('span', {'class': 'a-size-medium'}).text.strip() if product.find('span', {'class': 'a-size-medium'}) else \"-\"\n",
    "            product_price = product.find('span', {'class': 'a-price-whole'}).text.strip() if product.find('span', {'class': 'a-price-whole'}) else \"-\"\n",
    "            product_rating = product.find('span', {'class': 'a-icon-alt'}).text.strip() if product.find('span', {'class': 'a-icon-alt'}) else \"-\"\n",
    "            product_reviews = product.find('span', {'class': 'a-size-base'}).text.strip() if product.find('span', {'class': 'a-size-base'}) else \"-\"\n",
    "\n",
    "            print(\"Product:\", product_name)\n",
    "            print(\"Price:\", product_price)\n",
    "            print(\"Rating:\", product_rating)\n",
    "            print(\"Reviews:\", product_reviews)\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve search results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product_name):\n",
    "    base_url = 'https://www.amazon.in/s'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    all_products = []\n",
    "\n",
    "    for page_num in range(1, 4):\n",
    "        params = {'k': product_name, 'page': page_num}\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            products = soup.find_all('div', {'class': 'sg-col-inner'})\n",
    "\n",
    "            for product in products:\n",
    "                details = {}\n",
    "                details['Brand Name'] = product.find('span', {'class': 'a-size-base-plus a-color-base a-text-normal'}).text.strip() if product.find('span', {'class': 'a-size-base-plus a-color-base a-text-normal'}) else \"-\"\n",
    "                details['Name of the Product'] = product.find('span', {'class': 'a-size-base-plus a-color-base a-text-normal'}).text.strip() if product.find('span', {'class': 'a-size-base-plus a-color-base a-text-normal'}) else \"-\"\n",
    "                details['Price'] = product.find('span', {'class': 'a-price-whole'}).text.strip() if product.find('span', {'class': 'a-price-whole'}) else \"-\"\n",
    "                details['Return/Exchange'] = product.find('span', {'class': 'a-text-bold'}).text.strip() if product.find('span', {'class': 'a-text-bold'}) else \"-\"\n",
    "                details['Expected Delivery'] = product.find('span', {'class': 'a-text-bold'}).text.strip() if product.find('span', {'class': 'a-text-bold'}) else \"-\"\n",
    "                details['Availability'] = product.find('span', {'class': 'a-size-base'}).text.strip() if product.find('span', {'class': 'a-size-base'}) else \"-\"\n",
    "                details['Product URL'] = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal a-text-normal'})['href'] if product.find('a', {'class': 'a-link-normal a-text-normal'}) else \"-\"\n",
    "                \n",
    "                all_products.append(details)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve search results for page {page_num}\")\n",
    "\n",
    "    return all_products\n",
    "\n",
    "def save_to_csv(product_name, products):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(f'{product_name}_products.csv', index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "    products_data = scrape_product_details(user_input)\n",
    "    save_to_csv(user_input, products_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65134081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver.exe\")  # Provide path to your chromedriver\n",
    "\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    image_urls = set()\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if img.has_attr(\"src\"):\n",
    "            image_urls.add(img[\"src\"])\n",
    "\n",
    "    image_urls = [url for url in image_urls if not url.startswith(\"data:image\")]\n",
    "    os.makedirs(keyword, exist_ok=True)\n",
    "\n",
    "    for i, url in enumerate(image_urls[:num_images]):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            with open(os.path.join(keyword, f\"{keyword}_{i+1}.jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded image {i+1} for '{keyword}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image {i+1} for '{keyword}': {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images_per_keyword = 10\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for '{keyword}'...\")\n",
    "        scrape_images(keyword, num_images_per_keyword)\n",
    "        print()\n",
    "\n",
    "    print(\"Scraping complete.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9575863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(keyword):\n",
    "    base_url = f\"https://www.flipkart.com/search?q={keyword}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        results = []\n",
    "\n",
    "        for product in products:\n",
    "            details = {}\n",
    "            details['Brand Name'] = product.find('div', {'class': '_4rR01T'}).text.strip() if product.find('div', {'class': '_4rR01T'}) else \"-\"\n",
    "            details['Smartphone Name'] = product.find('a', {'class': 'IRpwTa'}).text.strip() if product.find('a', {'class': 'IRpwTa'}) else \"-\"\n",
    "            details['Colour'] = product.find('a', {'class': '_1WPlpC'}).text.strip() if product.find('a', {'class': '_1WPlpC'}) else \"-\"\n",
    "            details['RAM'] = [spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'}) if 'RAM' in spec.text.strip()][0] if any('RAM' in spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'})) else \"-\"\n",
    "            details['Storage(ROM)'] = [spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'}) if 'ROM' in spec.text.strip()][0] if any('ROM' in spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'})) else \"-\"\n",
    "            details['Primary Camera'] = [spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'}) if 'MP' in spec.text.strip() and 'Camera' in spec.text.strip()][0] if any('MP' in spec.text.strip() and 'Camera' in spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'})) else \"-\"\n",
    "            details['Secondary Camera'] = [spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'}) if 'MP' in spec.text.strip() and 'Front' in spec.text.strip()][0] if any('MP' in spec.text.strip() and 'Front' in spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'})) else \"-\"\n",
    "            details['Display Size'] = [spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'}) if 'inch' in spec.text.strip()][0] if any('inch' in spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'})) else \"-\"\n",
    "            details['Battery Capacity'] = [spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'}) if 'mAh' in spec.text.strip()][0] if any('mAh' in spec.text.strip() for spec in product.find_all('li', {'class': 'rgWa7D'})) else \"-\"\n",
    "            details['Price'] = product.find('div', {'class': '_30jeq3'}).text.strip() if product.find('div', {'class': '_30jeq3'}) else \"-\"\n",
    "            details['Product URL'] = 'https://www.flipkart.com' + product.find('a', {'class': 'IRpwTa'})['href'] if product.find('a', {'class': 'IRpwTa'}) else \"-\"\n",
    "            \n",
    "            results.append(details)\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        print(\"Failed to retrieve search results.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(keyword, results):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f'{keyword}_smartphones.csv', index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keyword = input(\"Enter the smartphone you want to search on Flipkart: \")\n",
    "    results = scrape_flipkart_smartphones(keyword)\n",
    "    if results:\n",
    "        save_to_csv(keyword, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "def scrape_coordinates(city_name):\n",
    "   \n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver.exe\")  \n",
    "    driver.maximize_window()\n",
    "\n",
    "   \n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "    search_bar = driver.find_element_by_css_selector(\"input[aria-label='Search Google Maps']\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(city_name)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "   current_url = driver.current_url\n",
    "\n",
    "     driver.quit()\n",
    "\n",
    " if \"/@\" in current_url:\n",
    "   coordinates_index = current_url.index(\"/@\") + 2\n",
    "    coordinates_string = current_url[coordinates_index:].split(\",\")[0]\n",
    "     latitude = coordinates_string.split(\",\")[0]\n",
    "        longitude = coordinates_string.split(\",\")[1]\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        print(\"Failed to scrape coordinates. Please try again.\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the name of the city to search on Google Maps: \")\n",
    "    latitude, longitude = scrape_coordinates(city)\n",
    "    if latitude and longitude:\n",
    "        print(f\"Coordinates for {city}: Latitude {latitude}, Longitude {longitude}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c57039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        laptops = soup.find_all(\"div\", class_=\"TopNumbeHeading sticky-footer\")\n",
    "        laptop_details = []\n",
    "\n",
    "        for laptop in laptops:\n",
    "            details = {}\n",
    "            details[\"Name\"] = laptop.find(\"div\", class_=\"heading-wraper\").text.strip()\n",
    "            details[\"Price\"] = laptop.find(\"td\", class_=\"smprice\").text.strip()\n",
    "            details[\"Specifications\"] = laptop.find(\"div\", class_=\"Section-center\").text.strip()\n",
    "            laptop_details.append(details)\n",
    "\n",
    "        return laptop_details\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops_data = scrape_gaming_laptops()\n",
    "    if gaming_laptops_data:\n",
    "        save_to_csv(gaming_laptops_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        billionaires = soup.find_all(\"div\", class_=\"personName\")\n",
    "        billionaires_data = []\n",
    "\n",
    "        for billionaire in billionaires:\n",
    "            details = {}\n",
    "            details[\"Rank\"] = billionaire.find_previous(\"div\", class_=\"rank\").text.strip()\n",
    "            details[\"Name\"] = billionaire.text.strip()\n",
    "            details[\"Net Worth\"] = billionaire.find_next(\"div\", class_=\"netWorth\").text.strip()\n",
    "            details[\"Age\"] = billionaire.find_next(\"div\", class_=\"age\").text.strip()\n",
    "            details[\"Citizenship\"] = billionaire.find_next(\"div\", class_=\"countryOfCitizenship\").text.strip()\n",
    "            details[\"Source\"] = billionaire.find_next(\"div\", class_=\"source-column\").text.strip()\n",
    "            details[\"Industry\"] = billionaire.find_next(\"div\", class_=\"category\").text.strip()\n",
    "            billionaires_data.append(details)\n",
    "\n",
    "        return billionaires_data\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_data = scrape_forbes_billionaires()\n",
    "    if billionaires_data:\n",
    "        save_to_csv(billionaires_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "def extract_comments(video_id, max_results=500):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    total_results = 0\n",
    "\n",
    "    while total_results < max_results:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=min(100, max_results - total_results),\n",
    "            pageToken=next_page_token if next_page_token else \"\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comment_time = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            comment_time = datetime.datetime.strptime(comment_time, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            comments.append({\"Comment\": comment, \"Time\": comment_time})\n",
    "            total_results += 1\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    comments = extract_comments(video_id)\n",
    "    print(f\"Total comments extracted: {len(comments)}\")\n",
    "    for idx, comment in enumerate(comments, start=1):\n",
    "        print(f\"\\nComment {idx}:\")\n",
    "        print(f\"Text: {comment['Comment']}\")\n",
    "        print(f\"Time: {comment['Time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        hostels = soup.find_all(\"div\", class_=\"fabresult\")\n",
    "        hostel_data = []\n",
    "\n",
    "        for hostel in hostels:\n",
    "            details = {}\n",
    "            details[\"Hostel Name\"] = hostel.find(\"h2\", class_=\"title-5\").text.strip()\n",
    "            details[\"Distance from City Centre\"] = hostel.find(\"span\", class_=\"description\").text.strip()\n",
    "            details[\"Ratings\"] = hostel.find(\"div\", class_=\"score orange big\").text.strip()\n",
    "            details[\"Total Reviews\"] = hostel.find(\"div\", class_=\"reviews\").text.strip().split()[0]\n",
    "            details[\"Overall Reviews\"] = hostel.find(\"div\", class_=\"keyword\").text.strip()\n",
    "            details[\"Privates from Price\"] = hostel.find(\"span\", class_=\"price title-5\").text.strip().split()[0]\n",
    "            details[\"Dorms from Price\"] = hostel.find(\"span\", class_=\"price\").text.strip().split()[0]\n",
    "            details[\"Facilities\"] = \", \".join([item.text.strip() for item in hostel.find_all(\"div\", class_=\"facilities\")])\n",
    "            details[\"Property Description\"] = hostel.find(\"div\", class_=\"rating-factors prop-card-tablet rating-factors small\").text.strip()\n",
    "            hostel_data.append(details)\n",
    "\n",
    "        return hostel_data\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"hostels_in_london.csv\", index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hostels_data = scrape_hostels_in_london()\n",
    "    if hostels_data:\n",
    "        save_to_csv(hostels_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef579a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
