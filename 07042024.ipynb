{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cabee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', class_='wikitable sortable')\n",
    "\n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:  \n",
    "        cells = row.find_all('td')\n",
    "        ranks.append(cells[0].text.strip())\n",
    "        names.append(cells[1].text.strip())\n",
    "        artists.append(cells[2].text.strip())\n",
    "        upload_dates.append(cells[3].text.strip())\n",
    "        views.append(cells[4].text.strip())\n",
    "\n",
    "    for i in range(len(ranks)):\n",
    "        print(\"Rank:\", ranks[i])\n",
    "        print(\"Name:\", names[i])\n",
    "        print(\"Artist:\", artists[i])\n",
    "        print(\"Upload Date:\", upload_dates[i])\n",
    "        print(\"Views:\", views[i])\n",
    "        print(\"\\n\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error fetching the webpage:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39db5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from the website.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_datasets_from_uci():\n",
    "    url = \"https://archive.ics.uci.edu/\"\n",
    "    show_all_datasets_url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "\n",
    "    response = requests.get(show_all_datasets_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        datasets_table = soup.find(\"table\", class_=\"table\")\n",
    "        dataset_rows = datasets_table.find_all(\"tr\")[1:] \n",
    "        datasets_data = []\n",
    "\n",
    "        for row in dataset_rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            dataset_name = columns[0].text.strip()\n",
    "            data_type = columns[1].text.strip()\n",
    "            task = columns[2].text.strip()\n",
    "            attribute_type = columns[3].text.strip()\n",
    "            num_instances = columns[4].text.strip()\n",
    "            num_attributes = columns[5].text.strip()\n",
    "            year = columns[6].text.strip()\n",
    "\n",
    "            datasets_data.append({\n",
    "                \"Dataset Name\": dataset_name,\n",
    "                \"Data Type\": data_type,\n",
    "                \"Task\": task,\n",
    "                \"Attribute Type\": attribute_type,\n",
    "                \"No of Instances\": num_instances,\n",
    "                \"No of Attributes\": num_attributes,\n",
    "                \"Year\": year\n",
    "            })\n",
    "\n",
    "        return datasets_data\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"datasets_from_uci.csv\", index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_data = scrape_datasets_from_uci()\n",
    "    if datasets_data:\n",
    "        save_to_csv(datasets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billboard_top_100():\n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver.exe\") \n",
    "    driver.maximize_window()\n",
    "\n",
    "    driver.get(\"https://www.billboard.com/\")\n",
    "    \n",
    "    charts_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.LINK_TEXT, \"Charts\")))\n",
    "    charts_option.click()\n",
    "\n",
    "    hot_100_link = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.LINK_TEXT, \"Hot 100\")))\n",
    "    hot_100_link.click()\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".chart-list\")))\n",
    "\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    chart_list = soup.find(\"ol\", class_=\"chart-list__elements\")\n",
    "\n",
    "    songs_data = []\n",
    "    for song in chart_list.find_all(\"li\", class_=\"chart-list__element\"):\n",
    "        song_details = {}\n",
    "        song_details[\"Song Name\"] = song.find(\"span\", class_=\"chart-element__information__song\").text.strip()\n",
    "        song_details[\"Artist Name\"] = song.find(\"span\", class_=\"chart-element__information__artist\").text.strip()\n",
    "        song_details[\"Last Week Rank\"] = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip() if song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\") else \"-\"\n",
    "        song_details[\"Peak Rank\"] = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "        song_details[\"Weeks on Board\"] = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "        songs_data.append(song_details)\n",
    "\n",
    "    return songs_data\n",
    "\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"billboard_top_100.csv\", index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_100_data = scrape_billboard_top_100()\n",
    "    if top_100_data:\n",
    "        save_to_csv(top_100_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_highest_selling_novels():\n",
    "    url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        novels_data = []\n",
    "        \n",
    "        table = soup.find(\"table\")\n",
    "        rows = table.find_all(\"tr\")[1:] \n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            book_name = columns[0].text.strip()\n",
    "            author_name = columns[1].text.strip()\n",
    "            volumes_sold = columns[2].text.strip()\n",
    "            publisher = columns[3].text.strip()\n",
    "            genre = columns[4].text.strip()\n",
    "\n",
    "            novels_data.append({\n",
    "                \"Book Name\": book_name,\n",
    "                \"Author Name\": author_name,\n",
    "                \"Volumes Sold\": volumes_sold,\n",
    "                \"Publisher\": publisher,\n",
    "                \"Genre\": genre\n",
    "            })\n",
    "\n",
    "        return novels_data\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"highest_selling_novels.csv\", index=False)\n",
    "    print(\"Data saved to CSV successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    novels_data = scrape_highest_selling_novels()\n",
    "    if novels_data:\n",
    "        save_to_csv(novels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    fixtures_link = soup.find('a', text='International Fixtures')['href']\n",
    "    fixtures_url = url + fixtures_link\n",
    "\n",
    "    fixtures_response = requests.get(fixtures_url)\n",
    "    fixtures_response.raise_for_status()  \n",
    "\n",
    "    fixtures_soup = BeautifulSoup(fixtures_response.text, 'html.parser')\n",
    "\n",
    "    fixtures = fixtures_soup.find_all('div', class_='fixture__format-strip')\n",
    "\n",
    "    for fixture in fixtures:\n",
    "        series = fixture.find('span', class_='u-unskewed-text').text.strip()\n",
    "        place = fixture.find('p', class_='fixture__additional-info').text.strip()\n",
    "        date = fixture.find('span', class_='fixture__datetime tablet-only').text.strip()\n",
    "        time = fixture.find('span', class_='fixture__time').text.strip()\n",
    "\n",
    "        print(\"Series:\", series)\n",
    "        print(\"Place:\", place)\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Time:\", time)\n",
    "        print(\"\\n\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error fetching the webpage:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def initialize_driver():\n",
    "   \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\") \n",
    "    driver_path = \"chromedriver.exe\"  \n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    return driver\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "try:\n",
    "    driver = initialize_driver()\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    explore_dropdown = driver.find_element(By.XPATH, \"//summary[contains(text(), 'Explore')]\")\n",
    "    explore_dropdown.click()\n",
    "\n",
    "\n",
    "    trending_option = driver.find_element(By.XPATH, \"//a[contains(text(), 'Trending')]\")\n",
    "    trending_option.click()\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"Box-row\")))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    repositories = soup.find_all('article', class_='Box-row')\n",
    "\n",
    "      for repo in repositories:\n",
    "        repo_title = repo.find('h1', class_='h3 lh-condensed').text.strip()\n",
    "        repo_description = repo.find('p', class_='col-9 color-text-secondary my-1 pr-4').text.strip()\n",
    "        contributors_count = repo.find('a', class_='Link--muted d-inline-block mr-3').text.strip()\n",
    "        language_used = repo.find('span', itemprop='programmingLanguage').text.strip()\n",
    "\n",
    "        print(\"Repository Title:\", repo_title)\n",
    "        print(\"Repository Description:\", repo_description)\n",
    "        print(\"Contributors Count:\", contributors_count)\n",
    "        print(\"Language Used:\", language_used)\n",
    "        print(\"\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "  \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    economy_link = soup.find('a', text='Economy')['href']\n",
    "    economy_url = url + economy_link\n",
    "\n",
    "    economy_response = requests.get(economy_url)\n",
    "    economy_response.raise_for_status()\n",
    "\n",
    "    economy_soup = BeautifulSoup(economy_response.text, 'html.parser')\n",
    "\n",
    "    india_gdp_link = economy_soup.find('a', text='India GDP')['href']\n",
    "    india_gdp_url = url + india_gdp_link\n",
    "\n",
    " \n",
    "    india_gdp_response = requests.get(india_gdp_url)\n",
    "    india_gdp_response.raise_for_status()  \n",
    "\n",
    "    india_gdp_soup = BeautifulSoup(india_gdp_response.text, 'html.parser')\n",
    "    \n",
    "    gdp_table = india_gdp_soup.find('table', class_='display dataTable')\n",
    "\n",
    "    for row in gdp_table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text.strip()\n",
    "        state = columns[1].text.strip()\n",
    "        gdp_18_19 = columns[2].text.strip()\n",
    "        gdp_19_20 = columns[3].text.strip()\n",
    "        share_18_19 = columns[4].text.strip()\n",
    "        gdp_billion = columns[5].text.strip()\n",
    "\n",
    "        print(\"Rank:\", rank)\n",
    "        print(\"State:\", state)\n",
    "        print(\"GSDP(18-19):\", gdp_18_19)\n",
    "        print(\"GSDP(19-20):\", gdp_19_20)\n",
    "        print(\"Share(18-19):\", share_18_19)\n",
    "        print(\"GDP($ billion):\", gdp_billion)\n",
    "        print(\"\\n\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error fetching the webpage:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edf359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36824911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984025d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
